\documentclass[sigconf,anonymous,review]{acmart}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

% ACM/SIGIR metadata (placeholders during review)
\acmConference[SIGIR '25]{Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval}{2025}{TBD}
\acmYear{2025}
\setcopyright{none}
\acmISBN{}
\acmDOI{}

\title{Causally Calibrated LLM-as-Judge for Recommenders: A Closed-Loop Framework for Exposure-Aware Evaluation and Synthetic Interaction Generation}
\author{Anonymous Author(s)}
\affiliation{\institution{Anonymous Submission} \country{}}
\date{}

\begin{document}

\section*{Contributions}
We propose a closed-loop, exposure-aware evaluation framework with the following contributions:
\begin{itemize}
  \item \textbf{Exposure-aware LLM-as-Judge} on matched item pairs, with \emph{evidence-aligned} rationales auditable by exact substring checks.
  \item \textbf{Causal estimators for metrics} (IPS/SNIPS/DR) with clipping/trimming and overlap diagnostics; user-level agreement (RJS) for \emph{user-fair} aggregation.
  \item \textbf{Monotonic causal calibration} from judge scores to IPS-Recall@\(K\), preserving order and improving offline--online alignment.
  \item \textbf{Exposure-controlled synthesis} that augments long-tail interactions under collaborative constraints, enabling an evaluate--synthesize--re-evaluate loop.
\end{itemize}

\begin{abstract}
Large language models (LLMs) are increasingly used as evaluators for recommender systems, but most \emph{LLM-as-Judge} paradigms ignore exposure bias, lack auditable evidence, and rarely connect offline judgments to online impact. We present a closed-loop framework that unifies: (i) an exposure-aware, rationale-grounded judge operating on matched pairs with verifiable evidence; (ii) an exposure-controlled synthesizer that fills tail data gaps under collaborative constraints; and (iii) a causal calibration link mapping judge scores to inverse-propensity (IPS) proxies of online lift via monotonic regression. Our judge enforces evidence alignment to curb hallucinations; our metrics use IPS/SNIPS/DR estimators with overlap diagnostics and sensitivity to clipping/trimming; and our synthesizer controls exposure while respecting sequence plausibility. On MovieLens-1M, we demonstrate a reproducible evaluate–synthesize–re-evaluate loop: stable IPS-weighted pairwise agreement, interpretable user-level agreement (RJS), exposure-controlled tail augmentation, and a monotonic calibration from RJS to IPS-Recall@K. We release code, configs, and reports to facilitate rigorous, evidence-audited, exposure-aware evaluation.
\end{abstract}

\ccsdesc[500]{Information systems~Recommender systems}
\ccsdesc[300]{Information systems~Evaluation of retrieval results}

\keywords{recommender systems, LLM-as-Judge, exposure bias, IPS/SNIPS/DR, isotonic calibration, synthetic interactions}

\maketitle

\section{Introduction}
Traditional offline metrics (e.g., HR/NDCG/AUC) depend on exposure distributions and negative sampling protocols, leading to blind spots in long-tail, cold-start, cross-domain, and user experience evaluation. Prior \emph{LLM-as-Judge} often rely on subjective scoring without modeling exposure bias or externally validating to online impact. Real logs are sparse in the tail, limiting training and generalization. LURE aims at an exposure-aware, rationale-grounded judge paired with an exposure-controllable synthesizer to form a \emph{evaluate–synthesize–re-evaluate} loop for fairer offline evaluation and tail-aware training.

\section{Problem Setup and Method Overview}
\paragraph{Exposure bias in recommenders.} Logged interactions reflect a product of user preference and platform exposure. Offline metrics computed under the logged exposure distribution can be misaligned with counterfactual performance. IPS and doubly-robust (DR) estimators are standard tools for causal unbiasing in recommendation and bandits.

\paragraph{Overview.} LURE comprises: (i) \textbf{RecJudge-X}, an exposure-aware LLM judge with matched pairs, IPS weighting, and evidence-aligned rationales; (ii) \textbf{ExpoSynth}, a constraint-driven synthesizer that targets a desired exposure curve; and (iii) \textbf{Calib-Link}, an isotonic (monotonic) regression \(g\) mapping judge scores to IPS proxies or online metrics.

\subsection*{Assumptions and Overlap}
We estimate exposure propensity \(\hat{\pi}(i\mid u)\) from observational features and rely on standard causal assumptions: consistency, positivity (overlap), and no unmeasured confounding for exposure given features. We report overlap diagnostics (min/quantiles and mass below thresholds) and conduct clipping/trimming sensitivity to mitigate finite-sample instability.

\subsection{RecJudge-X}
\paragraph{Input representation.} For each user \(u\), we construct a history block (titles, genres, and meta bins) and candidate item blocks. The prompt enforces a strict JSON schema with a field \texttt{winner\;\in\;\{i,j,tie\}}, confidence \(p \in [0,1]\), and a list of \texttt{rationales} each containing an \texttt{evidence} substring that must exactly match the input. We parse the output, tolerate minor formatting errors, and verify that every evidence span is a substring of the prompt; non-conforming instances are down-weighted or discarded.

\paragraph{Matched pairs.} To reduce confounding from exposure correlates, we sample pairs \((i,j)\) matched on popularity and age bins (and optionally price or category). Let \(\hat{\pi}(i\mid u)\) denote the estimated exposure propensity (Section~\ref{sec:exposure}). For a judged pair \((u,i,j)\), we define the IPS weight
\begin{equation}
  w(u,i,j) \;=\; \frac{1}{\max(\hat{\pi}(i\mid u),\,10^{-6})} \, + \, \frac{1}{\max(\hat{\pi}(j\mid u),\,10^{-6})} .
\end{equation}

\paragraph{IPS-PairAUC.} Let \(\widehat{\Pr}[i\succ j\mid u]\) be the model’s binary prediction based on scores \(s(i), s(j)\) (or the LLM confidence with a 0.5 threshold), and let the LLM judge return a label \(y\in\{+1,-1,0\}\) indicating agreement (+1 if \(i\) preferred, -1 if \(j\), 0 if tie/invalid). We compute
\begin{equation}
  \mathrm{PairAUC}_{\text{IPS}} \;=\; \frac{\sum_{(u,i,j)} w(u,i,j)\, \mathbf{1}[(\widehat{\Pr}[i\succ j]\ge 1/2 \land y=+1)\,\lor\,(\widehat{\Pr}[i\succ j]<1/2 \land y=-1)]}{\sum_{(u,i,j)} w(u,i,j)}.
\end{equation}

\paragraph{RJS (exposure-aware Kendall-like \(\tau_w\)).} For each user, compute agreement \(a_u = \frac{\sum w\,\mathrm{match}}{\sum w}\), then map to \(\tau_u = 2a_u - 1\in[-1,1]\). Finally aggregate by propensity weights: \(\tau_w = \frac{\sum_u (\sum w)_u\,\tau_u}{\sum_u (\sum w)_u}\). This yields an interpretable, rationale-grounded, exposure-aware ranking score.

\subsection{ExpoSynth}
\label{sec:exposynth}
\paragraph{Collaborative constraints.} From logs we estimate: (i) a genre transition matrix \(T[g\to g']\) from consecutive interactions; and (ii) an item-item similarity graph via cosine similarity of the item–user incidence matrix. We accept a step \(i\to j\) if either \(T[g(i)\to g(j)]\) exceeds a threshold or similarity \(S(i,j)\) exceeds \(\delta\).

\paragraph{Exposure control.} We define a target exposure curve \(q(\mathrm{pop\_bin})\) that upweights tail (e.g., pop\_bin\(\ge 3\)). Generation samples candidates from a mixture of anchor genres and \(q\), with rejection sampling under constraints.

\paragraph{Anchors.} For each user we form top-\(k\) anchors by genre frequency in recent history, using weights proportional to within-user prevalence. We iterate \(T\) steps per user, logging \((u,i,t,\mathrm{source}=\text{\tt synth}, \mathrm{reason}=\text{\tt anchor\mbox{-}based})\).

\subsection{Calib-Link}
\label{sec:calib}
Given tuples \(\{(M_k, \mathrm{RJS}_k, \widehat{\Delta \mathrm{Recall}}^{\mathrm{IPS}}_{k})\}_k\), we fit an isotonic regressor \(g\) such that \(g(\mathrm{RJS})\approx \widehat{\Delta \mathrm{Recall}}^{\mathrm{IPS}}\). We report Spearman \(\rho\), Kendall \(\tau\), and \(R^2\). Isotonicity preserves ordering and improves extrapolation stability. We compute IPS/SNIPS/DR variants with clipping/trimming sweeps and user-level bootstrap to form confidence intervals.

\section{Experimental Protocol and Reproducibility}
We use MovieLens-1M. Propensity \(\hat{\pi}\) is estimated with a logistic model over user frequency, item popularity, item age bin, hour, weekday, and user-genre entropy. Pair sampling matches popularity/age bins. Judge is executed with Tongyi (DashScope) via OpenAI-compatible or native APIs; rationales are validated by substring checks. Scripts, configs, and outputs are under version control; see \texttt{configs/movielens1m.yaml} and \texttt{reports/}.

\section{Results}

\subsection{Judge Metrics (RJS and IPS-PairAUC)}
Table~\ref{tab:rjs} summarizes overall and per-popularity-bucket results.

\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
Bucket & PairAUC\_IPS & RJS\\
\midrule
ALL & 0.445 & -0.110 \\
pop\_bin=0 & 0.378 & -0.243 \\
pop\_bin=1 & 0.500 & 0.000 \\
pop\_bin=2 & 0.488 & -0.024 \\
pop\_bin=3 & 0.434 & -0.132 \\
pop\_bin=4 & 0.419 & -0.161 \\
\bottomrule
\end{tabular}
\caption{RJS and IPS-PairAUC on MovieLens-1M prototype.}
\label{tab:rjs}
\end{table}

\subsection{Calibration (RJS to IPS-Recall@20)}
Figure~\ref{fig:calib} shows isotonic calibration g mapping RJS to IPS-Recall@20 using simulated models. In this small pilot, recall values saturate at 1.0, yielding a flat curve; larger and more diverse models increase variance and informative calibration.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{../reports/figs/calib_isotonic.png}
\caption{Isotonic calibration g: RJS \(\to\) IPS-Recall@20.}
\label{fig:calib}
\end{figure}

\subsection{ExpoSynth Tail Augmentation}
Our synthesizer produced 47,117 accepted interactions (accept ratio 0.98) with tail ratio 0.607 (pop\_bin\(\ge\)3). Constraints are enforced via the learned genre transition matrix and item similarities. Increasing steps-per-user proportionally scales synthetic volume while preserving tail emphasis.

\section{Related Work}
LLM-based evaluation for generation and recommendation has explored rubric-guided or pairwise judging, yet typically without explicit exposure correction or evidence auditing. Causal unbiasing in recommendation relies on IPS/DR and counterfactual estimators. Synthetic data for recommenders ranges from heuristic simulators to generative models, but few works control exposure explicitly while enforcing collaborative constraints.

\section{Theoretical Notes}
Under standard assumptions (consistency, positivity, and correct propensity), IPS yields an unbiased estimate of counterfactual averages. Our pairwise IPS objective inherits this property when matches isolate the treatment of interest (content differences) and propensity pertains to exposure, not preference. Monotone calibration via isotonic regression is consistent for the regression function under weak conditions and preserves order, which is critical for model ranking.

\section{Ablations and Robustness Protocol}
We recommend: (i) \emph{With/without exposure weights} to quantify bias reduction; (ii) \emph{With/without evidence checking} to evaluate hallucination control; (iii) \emph{Single vs.~multi-judge} to measure variance; (iv) \emph{Constraint on/off} in ExpoSynth to assess sequence plausibility and downstream gains; (v) Sensitivity to pair-matching criteria and LLM temperature.

\section{Ethics and Limitations}
LURE requires careful handling of privacy and potential demographic bias. Evidence alignment reduces, but does not eliminate, hallucination risks. Propensity mis-specification and positivity violations may bias IPS. Synthetic data should not be treated as real user behavior and must be audited before deployment.

\section{Discussion}
We unify causal unbiasing, verifiable rationales, and exposure-controllable synthesis. Future work includes multi-judge aggregation, stronger DR estimators, and cross-domain calibration.

\section{Conclusion}
LURE delivers an exposure-aware LLM judge with rationales, a controllable synthesizer for tail augmentation, and a calibration link to external metrics. The framework is reproducible and extensible.

\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem{IPS} A. Swaminathan and T. Joachims. Counterfactual Risk Minimization. KDD, 2015.
\bibitem{DR} M. Dud\'\i\v{k} et al. Doubly Robust Policy Evaluation and Learning. ICML, 2011.
\bibitem{Isotonic} B. Zadrozny and C. Elkan. Transforming Classifier Scores into Accurate Multiclass Probability Estimates. KDD, 2002.
\bibitem{BanditsRec} L. Li et al. A Contextual-Bandit Approach to Personalized News Article Recommendation. WWW, 2010.
\bibitem{LLMEval} OpenAI. Models Evaluating Models. 2023.
\end{thebibliography}

\end{document}


